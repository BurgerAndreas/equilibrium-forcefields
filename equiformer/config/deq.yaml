# @package _global_
# ^^^ this @package directive solves any nesting problem (if this file is included in another folder)

# call:
# scripts/deq_equiformer.py wandb=False +preset=md17_aspirin_sel2.yaml

defaults:
  # if _self_ is the first entry, compositions will overwrite this config
  # if _self_ is the last entry, this config will overwrite compositions (default)
  # https://hydra.cc/docs/1.3/upgrades/1.0_to_1.1/default_composition_order/
  - md17
  - _self_

override_test: 2

model_is_deq: True
# batch_size: 16

# reuse fixed-point of previous step
# DEQ Optical Flow used FPR only during inference
fpreuse_test: False
limit_f_max_iter_fpreuse: False

# meas_force: False
# should be the same, but custom autograd might interfere with torchdeq grad
noforcemodel: False

# passed into model
model_kwargs:
  deq_mode: True
  num_layers: 2
  # weight, spectral, both, or none
  # https://github.com/locuslab/torchdeq/blob/4f6bd5fa66dd991cad74fcc847c88061764cf8db/torchdeq/norm/base_norm.py#L90C23-L90C32
  torchdeq_norm: 
    norm_type: 'weight_norm' # weight_norm, spectral_norm
    clip: False # norm_clip
    clip_value: 1.0
    # dims (list or int, optional) – The dimensions along which to normalize.
    # learn_scale (bool, optional) – If true, learn a scale factor during training. Default True.
    # target_norm (float, optional) – The target norm value. Default 1.
    # clip (bool, optional) – If true, clip the scale factor. Default False.
    # clip_value (float, optional) – The value to clip the scale factor to. Default 1.

  input_injection: 'first_layer'
  # "FFResidualFCTPProjection", "FFProjection", "FCTPProjection"
  dec_proj: null
  # initialize fixed-point. zero, one, uniform, normal_mean_std e.g. normal_0.0_0.5
  z0: 'zero'
  z0_requires_grad: False
  # logging
  log_fp_error_traj: False


# passed to torchdeq
deq_kwargs:
  # https://torchdeq.readthedocs.io/en/latest/torchdeq/core.html#torchdeq.core.get_deq
  # 'anderson', 'broyden', 'fixed_point_iter', 'simple_fixed_point_iter'
  # (str, optional) – The forward solver function. Default 'fixed_point_iter'
  f_solver: 'fixed_point_iter'
  # (str, optional) – The backward solver function. Default 'fixed_point_iter'.
  b_solver: 'fixed_point_iter'
  # f_tol (float, optional) – The forward pass solver stopping criterion. Default 1e-3.
  f_tol: 1e-3
  # b_tol (float, optional) – The backward pass solver stopping criterion. Default 1e-6.
  b_tol: 1e-6
  # f_max_iter (int, optional) – Maximum number of iterations (NFE) for the forward solver. Default 40.
  f_max_iter: 40
  # b_max_iter (int, optional) – Maximum number of iterations (NFE) for the backward solver. Default 40
  b_max_iter: 40
  # ift (bool, optional) – If true, enable Implicit Differentiation. 
  # IFT=Implicit Function Theorem. Default False.
  ift: False
  # grad (Union[int, list[int], tuple[int]], optional) 
  # Specifies the steps of PhantomGrad. It allows for using multiple values 
  # to represent different gradient steps in the sampled trajectory states. 
  # Default 1.
  grad: 1

  # regularization
  # jac_reg: False
  # jac_loss_weight: 0.0
  # jac_loss_freq: 0.0
  # jac_incremental: 0.0
  # torchdeq.loss.jac_reg(f0, z0, vecs=1, create_graph=True)

# @package _global_
# ^^^ this @package directive solves any nesting problem (if this file is included in another folder)

# call:
# equiformer/main_md17.py wandb=False +preset=md17_aspirin_sel2.yaml

defaults:
  # if _self_ is the first entry, compositions will overwrite this config
  # if _self_ is the last entry, this config will overwrite compositions (default)
  # https://hydra.cc/docs/1.3/upgrades/1.0_to_1.1/default_composition_order/
  # - preset: md17_defaults
  - _self_

override_test: 1

# passed into model
model_kwargs:
  # (int)
  num_layers: 6
  activation: "SiLU" 
  num_basis: 128
  atomref: null
  irreps_node_attr: "1x0e"
  basis_type: "exp"
  irreps_pre_attn: null
  rescale_degree: False
  nonlinear_message: False
  norm_layer: "layer"
  # most import for parameter count?
  fc_neurons: [64, 64]
  irreps_node_embedding_injection: "64x0e+32x1e+16x2e"
  irreps_node_embedding: "128x0e+64x1e+32x2e"
  irreps_feature: "512x0e"  # scalars only
  irreps_sh: "1x0e+1x1e+1x2e"
  irreps_head: "32x0e+16x1e+8x2e"
  num_heads: 4
  irreps_mlp_mid: "384x0e+192x1e+96x2e"
  # regularization
  alpha_drop: 0.0
  proj_drop: 0.0
  out_drop: 0.0
  # drop_path: 0.0
  drop_path_rate: 0.0
  scale: null
  # (str)
  irreps_in: null
  # (float)
  max_radius: 5.0
  # normalization for TensorProduct (see o3 library)
  # https://docs.e3nn.org/en/stable/api/o3/o3_tp.html#e3nn.o3.TensorProduct
  dp_tp_path_norm: "none" # element, path
  dp_tp_irrep_norm: null # None = 'element'
  fc_tp_path_norm: "none"
  fc_tp_irrep_norm: null # None = 'element'

# use revised version of MD17 with more accurate energies and forces (bool)
md17revised: False
use_revised_splits: False
# use the non-revised (old) data downloaded from the revised dataset 
# and processed with the revised dataset's preprocessing script (bool)
md17revised_old: False

use_original_datasetcreation: False

# (str)
output_dir: null
# (str)
# dot_product_attention_transformer_exp_l2_md17
# model_name="graph_attention_transformer_nonlinear_l2_md17" # paper
# model_name: "graph_attention_transformer_nonlinear_l2_md17"
model_name: "dot_product_attention_transformer_exp_l2_md17"
# (int)
epochs: 1000
# (int)
batch_size: 8
# (int)
eval_batch_size: 24
# (int)
model_ema: False
# (float)
model_ema_decay: 0.9999
#      (float)
model_ema_force_cpu: False
# 'Optimizer (default: adamw' (str)
opt: "adamw"
# Optimizer Epsilon (default: 1e-8 (float)
opt_eps: 1e-8
# Optimizer Betas (default: None (float)
opt_betas: null
# Clip gradient norm (default: None (float)
clip_grad: null
# SGD momentum (default: 0.9 (float)
momentum: 0.9
# weight decay (default: 5e-3 (float) # 1e-6 for paper
weight_decay: 5e-3 
# 'LR scheduler (default: cosine' (str)
sched: "cosine"
# learning rate (default: 5e-4 or 1.5e-4) (float)
lr: 5e-4
# learning rate noise on/off epoch percentages (float)
lr_noise: null
# learning rate noise limit percent (default: 0.67 (float)
lr_noise_pct: 0.67
# learning rate noise std-dev (default: 1.0 (float)
lr_noise_std: 1.0
# warmup learning rate (default: 1e-6 (float)
warmup_lr: 1e-6
# lower lr bound for cyclic schedulers that hit 0 (1e-6 (float)
min_lr: 1e-6
# epoch interval to decay LR (float)
decay_epochs: 30
# epochs to warmup LR (int)
warmup_epochs: 10
# epochs to cooldown LR at min_lr (int)
cooldown_epochs: 10
# patience epochs for Plateau LR scheduler (default: 10 (int)
patience_epochs: 10
# LR decay rate (default: 0.1 (float)
decay_rate: 0.1
# (str)
target: "aspirin"
# (str)
data_path: "datasets/md17"
# size of training dataset (int)
train_size: 950
# size of validation dataset (int)
val_size: 50
# (int)
compute_stats: False
# epoch interval to evaluate on the testing set (int)
test_interval: 10
# max iteration to evaluate on the testing set (int)
test_max_iter: 1000
# after training is done (-1 means whole test set ~ 100-200k) (int)
test_max_iter_final: -1
# (float)
energy_weight: 0.2 # 1
# (float)
force_weight: 0.8 # 80
# (int)
seed: 1
# (int)
workers: 4
# Pin CPU memory in DataLoader for more efficient (sometimes (int)
pin_mem: False
# (int)
no_pin_mem: False
# load checkpoint (str)
checkpoint_path: null
# (bool)
save_best_checkpoint: False
save_periodic_checkpoint: False
save_final_checkpoint: False
# (str)
evaluate: False
# Include force in loss calculation. (str2bool)
meas_force: True

# use model.eval() during evaluation (bool)
eval_mode: False

# logging
# (int)
print_freq: 100
# major stuff (int)
log_every_step_major: 1000
# minor stuff (int)
log_every_step_minor: 100

wandb: True
wandb_run_name: null
wandb_group: null
slurm_job_id: null
model_is_deq: False
fpreuse_test: False

# wandb.watch gradients, activations, model parameters (bool)
watch_model: False

# variables we can access in our code
job_name: 'results'
# job_name: ${hydra:job.name}
config_name: ${hydra:job.config_name}
# Stores the command line arguments overrides
override_dirname: ${hydra:job.override_dirname}
# Changes the current working directory to the output directory for each job
# hydra.job.chdir: False

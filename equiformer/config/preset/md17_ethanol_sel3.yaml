# @package _global_
# ^^^ this @package directive solves any nesting problem (if this file is included in another folder)

# auto-generated from equiformer/scripts/train/md17/equiformer/se_l3/target@ethanol.sh

output_dir: 'models/md17/equiformer/se_l3/target@ethanol/lr@2e-4_bs@5_wd@1e-6_epochs@2000_w-f2e@80_dropout@0.0_exp@32_l2mae-loss'
model_name: 'graph_attention_transformer_nonlinear_exp_l3_md17'
target: 'ethanol'
data_path: 'datasets/md17'
epochs: 2000
lr: 2e-4
batch_size: 5
eval_batch_size: 16
weight_decay: 1e-6
energy_weight: 1
force_weight: 80


# auto-generated from equiformer/nets/graph_attention_transformer_md17.py
model_kwargs:
  atomref: null
  num_layers: 6
  irreps_node_embedding: "128x0e+64x1e+64x2e+32x3e"
  irreps_node_attr: "1x0e"
  irreps_sh: "1x0e+1x1e+1x2e+1x3e"
  fc_neurons: [64,64]
  basis_type: "exp"
  irreps_feature: "512x0e"
  irreps_head: "32x0e+16x1e+16x2e+8x3e"
  num_heads: 4
  irreps_pre_attn: null
  rescale_degree: False
  nonlinear_message: True
  irreps_mlp_mid: "384x0e+192x1e+192x2e+96x3e"
  norm_layer: "layer"
  alpha_drop: 0.0
  proj_drop: 0.0
  out_drop: 0.0
  drop_path_rate: 0.0
  scale: null
  irreps_in: '64x0e'
  number_of_basis: 32

# @package _global_
# ^^^ this @package directive solves any nesting problem (if this file is included in another folder)

model:
  # Statistics of QM9 with cutoff max_radius = 5
  # For simplicity, use the same statistics for MD17
  _AVG_NUM_NODES: 18.03065905448718
  _AVG_DEGREE: 15.57930850982666

# [md17, rmd17og, rmd17, md22]
# rmd17: use revised version of MD17 with more accurate energies and forces 
# rmd17og: use the non-revised (old) data but downloaded from the revised dataset (should be the same as md17)
dname: "md17"
use_original_datasetcreation: False # will only work with dname=md17
# (str)
data_path: "datasets"
# http://www.sgdml.org/#datasets
target: "aspirin"
# (str)
# output_dir: models/md17/equiformer/test
output_dir: auto
# if to overwrite _AVG_NUM_NODES and _AVG_DEGREE with the dataset statistics (bool)
load_stats: False
# Use molecule of dataset average statistics. only has an effect if load_stats is True
use_dataset_avg_stats: False 

# how to split the dataset into training, validation, and testing
# equiformer # default. randomly shuffles dataset into train, val, test without any overlap
# fpreuse_overlapping # consecutive across batches, test split overlaps with train split, train and val splits are the same as in Equiformer
# fpreuse_ordered # dataset is split into train, val, test without any shuffling. Testset is reordered to be consecutive across batches
# rmd17 # datasplit indices delivered with the revised MD17 dataset, only works with md17 or rmd17
datasplit: "equiformer"

# (int)
epochs: 1000
# if you want to reduce the number of training epochs without affecting how the learning rate is decayed (int)
max_epochs: ${epochs}
# If you want multiple runs over the dataset per epoch, e.g. for overfitting a single batch (int)
epochs_per_epochs: 1
# (int)
batch_size: 8
# (int)
eval_batch_size: 24
# (int)
model_ema: False
# (float)
model_ema_decay: 0.9999
# (float)
model_ema_force_cpu: False

# 'Optimizer (default: adamw' (str)
opt: "adamw"
# Optimizer Epsilon (default: 1e-8 (float)
opt_eps: 1e-8
# Optimizer Betas (default: None (float)
opt_betas: null
# Clip gradient norm (default: None (float)
clip_grad: null
# SGD momentum (default: 0.9 (float)
momentum: 0.9
# weight decay (default: 5e-3 (float) # 1e-6 for paper
weight_decay: 5e-3 
# when multiplying the batch size by k, one should multiply the learning rate by sqrt(k)
bsscale: null

# 'LR scheduler (default: cosine' (str)
sched: "cosine"
# learning rate (default: 5e-4 or 1.5e-4) (float)
lr: 5e-4
# learning rate noise on/off epoch percentages (float)
lr_noise: null
# learning rate noise limit percent (default: 0.67 (float)
lr_noise_pct: 0.67
# learning rate noise std-dev (default: 1.0 (float)
lr_noise_std: 1.0
# warmup learning rate (default: 1e-6 (float)
warmup_lr: 1e-6
# lower lr bound for cyclic schedulers that hit 0 (1e-6 (float)
min_lr: 1e-6
# epoch interval to decay LR (float)
decay_epochs: 30
# epochs to warmup LR (int)
warmup_epochs: 10
# epochs to cooldown LR at min_lr (int)
cooldown_epochs: 10
# patience epochs for Plateau LR scheduler (default: 10 (int)
patience_epochs: 10
# LR decay rate (default: 0.1 (float)
decay_rate: 0.1
# size of training dataset (int)
train_size: 950
# size of validation dataset (int)
val_size: 50
compute_stats: False
# epoch interval to evaluate on the testing set (int)
test_interval: 10
# max iteration to evaluate on the testing set (int)
test_max_iter: 1000
# after training is done (-1 means whole test set ~ 100-200k) (int)
test_max_iter_final: -1
#
loss_energy: l2mae
loss_force: l2mae
# loss multiplier
energy_weight: 0.2 # 1
force_weight: 0.8 # 80
# (int)
seed: 1
# (int)
workers: 4
# Pin CPU memory in DataLoader for more efficient (sometimes (int)
pin_mem: True
# (int)
no_pin_mem: True
num_workers: 4

# load checkpoint (str)
checkpoint_path: auto
save_best_test_checkpoint: False
save_best_val_checkpoint: False
save_checkpoint_after_test: True
save_final_checkpoint: False
max_checkpoints: 2
# (str)
evaluate: False
# Include force in loss calculation. (str2bool)
meas_force: True
clip_grad_norm: False # False, 5.0, 100

# use model.eval() during evaluation (bool)
test_w_eval_mode: True
# use torch.no_grad() during evaluation (bool)
test_w_grad: False

normalizer: md17 # md17 or oc20

# logging
# (int)
print_freq: 100
# major stuff (int)
log_every_step_major: 1000
# minor stuff (int)
log_every_step_minor: 100

# if to stop after one forward pass (bool)
test_forward: False
wandb: True
wandb_run_name: null
wandb_group: null
slurm_job_id: null
model_is_deq: False

# reuse fixed-point of previous step
# DEQ Optical Flow used FPR only during inference
fpreuse_test: False

# (sparse) fixed-point correction
fpc_loss: l2
fpc_freq: 0 # how many fixed-point estimates to pick for the loss. -1 means all, 0 means none. 1-4 is reasonable
fpc_rand: False # if to pick fixed-point estimates unform randomly or uniformly spaced
fpc_weight: 0.8 # weight for the fixed-point correction loss
fpc_wfunc: const # weighting of loss in time. 'const' (constant), 'linear', and 'exp'

# contrastive fixed-point loss
contrastive_loss: False # False, next, triplet
contrastive_weight: 0.8 # weight for the contrastive fixed-point loss
contrastive_w_grad: False # if to use gradients in the contrastive fixed-point loss
tripletloss_margin: 0.0 # margin for triplet loss

# fixed-point reuse loss
fpr_loss: False
fpr_w_eval: False
fpr_w_grad: False
fpr_weight: 1.0

unsqueeze_e_dim: False # if to reshape model output [B] (OC20) -> [B,1] (MD17)
squeeze_e_dim: False # if to reshape data [B,1] (MD17) -> [B] (OC20)

# wandb.watch gradients, activations, model parameters (bool)
watch_model: False

# variables we can access in our code
job_name: 'results'
# job_name: ${hydra:job.name}
config_name: ${hydra:job.config_name}
# Stores the command line arguments overrides
override_dirname: ${hydra:job.override_dirname}
# Changes the current working directory to the output directory for each job
# hydra.job.chdir: False
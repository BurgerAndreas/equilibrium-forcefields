# @package _global_
# ^^^ this @package directive solves any nesting problem (if this file is included in another folder)

# call:
# scripts/deq_equiformer.py wandb=False +preset=md17_aspirin_sel2.yaml

defaults:
  # if _self_ is the first entry, compositions will overwrite this config
  # if _self_ is the last entry, this config will overwrite compositions (default)
  # https://hydra.cc/docs/1.3/upgrades/1.0_to_1.1/default_composition_order/
  # - md17
  - _self_

override_test: e1deq

model_is_deq: True
# batch_size: 16

# debugging tools
# meas_force: False
# should be the same as meas_force, but custom autograd might interfere with torchdeq grad
noforcemodel: False

# passed into model
model:
  deq_mode: True
  num_layers: 2
  # weight, spectral, both, or none
  # https://github.com/locuslab/torchdeq/blob/4f6bd5fa66dd991cad74fcc847c88061764cf8db/torchdeq/norm/base_norm.py#L90C23-L90C32
  torchdeq_norm: 
    norm_type: weight_norm # None weight_norm, spectral_norm
    # dims (list or int, optional) – The dimensions along which to normalize.
    learn_scale: True # (bool, optional) – If true, learn a scale factor during training. Default True.
    target_norm: 1.0 # (float, optional) – The target norm value. Default 1.
    clip: False # (bool, optional) – If true, clip the scale factor. Default False.
    clip_value: 1.0 # (float, optional) – The value to clip the scale factor to. Default 1.
  

  input_injection: 'first_layer'
  # True: concat input injection onto node features. False: use addition instead.
  cat_injection: True
  norm_injection: null # None=False, 'one', 'prev'
  # "FFResidualFCTPProjection", "FFProjection", "FCTPProjection"
  dec_proj: null
  # initialize fixed-point. zero, one, uniform, normal_mean_std e.g. normal_0.0_0.5
  z0: 'zero'
  z0_requires_grad: False
  # logging
  log_fp_error_traj: False
  # debugging: instead of fixed-point pass initial features to the decoder
  skip_implicit_layer: False
  # weight_init_blocks='{EquivariantLayerNormV2_w:1,ParameterList:normal_0.0_0.5}'
  weight_init: null
  weight_init_blocks: null
  # DPA, DPANorm, FF, FFNorm, FFNormResidual
  deq_block: null

deq_kwargs_test:
  # on test set only when passed a fixed-point estimate
  fpreuse_f_max_iter: _default
  fpreuse_f_tol: _default
  # on test set
  f_max_iter: _default
  f_tol: _default

# passed to torchdeq
deq_kwargs:
  # https://torchdeq.readthedocs.io/en/latest/torchdeq/core.html#torchdeq.core.get_deq
  # 'anderson', 'broyden', 'fixed_point_iter', 'simple_fixed_point_iter'
  # (str, optional) – The forward solver function. Default 'fixed_point_iter'
  f_solver: 'broyden'
  # (str, optional) – The backward solver function. Default 'fixed_point_iter'.
  b_solver: 'fixed_point_iter'
  # f_tol (float, optional) – The forward pass solver stopping criterion. Default 1e-3.
  f_tol: 1e-3
  # b_tol (float, optional) – The backward pass solver stopping criterion. Default 1e-6.
  b_tol: 1e-6
  # f_max_iter (int, optional) – Maximum number of iterations (NFE) for the forward solver. Default 40.
  f_max_iter: 40
  # b_max_iter (int, optional) – Maximum number of iterations (NFE) for the backward solver. Default 40
  b_max_iter: 40
  # ift (bool, optional) – If true, enable Implicit Differentiation. 
  # IFT=Implicit Function Theorem. Default False.
  ift: False
  hook_ift: False
  # grad (Union[int, list[int], tuple[int]], optional) 
  # Specifies the steps of PhantomGrad. It allows for using multiple values 
  # to represent different gradient steps in the sampled trajectory states. 
  # Default 1.
  grad: 1

  # regularization
  # jac_reg: False
  # jac_loss_weight: 0.0
  # jac_loss_freq: 0.0
  # jac_incremental: 0.0
  # torchdeq.loss.jac_reg(f0, z0, vecs=1, create_graph=True)

# (str)
output_dir: models/md17/deq/test
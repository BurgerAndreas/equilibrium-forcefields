# @package _global_
# ^^^ this @package directive solves any nesting problem (if this file is included in another folder)

defaults:
  # if _self_ is the first entry, compositions will overwrite this config
  # if _self_ is the last entry, this config will overwrite compositions (default)
  # https://hydra.cc/docs/1.3/upgrades/1.0_to_1.1/default_composition_order/
  - preset: tiny_l3
  - data: 200k
  - _self_

# TODO
model:
  # Statistics of IS2RE 100K
  # IS2RE: 100k, max_radius = 5, max_neighbors = 100
  _AVG_NUM_NODES: 77.81317
  _AVG_DEGREE: 23.395238876342773

####################################################
# from ocp/ocpmodels/common/flags.py:

# Whether to train the model, make predictions, or to run relaxations
# choices=["train "predict "run-relaxations "validate"]
mode: "train"
config_yml: "" # Path to a config file listing data, model, optim parameters.
identifier: "" # Experiment identifier to append to checkpoint/log/result directory
debug: False # Whether this is a debugging run or not
run_dir: "./" # Directory to store checkpoint/log/result directory
print_every: 10 # Log every N iterations (default: 10)
seed: 0 # Seed for torch, cuda, numpy
amp: False # Use mixed-precision training
checkpoint: null # Model checkpoint to load
timestamp_id: null # Override time stamp ID. Useful for seamlessly continuing model training in logger.
eval_mode: False # use model.eval() during evaluation (bool)

# Cluster args
sweep_yml: null # Path to a config file with parameter sweeps
submit: False # Submit job to cluster
summit: False # Running on Summit cluster
logdir: "logs" # Where to store logs
slurm_partition: "ocp" # Name of partition
slurm_mem: 80 # Memory (in gigabytes)
slurm_timeout: 72 # Time (in hours)
num_gpus: 1 # int: Number of GPUs to request
distributed: False # Run with DDP
cpu: False # Run CPU only training
num_nodes: 1 # Number of Nodes to request
distributed_port: 13356 # Port on master for DDP
distributed_backend: "nccl" # Backend for DDP
local_rank: 0 # Local rank
no_ddp: False # Do not use DDP
# int: Number of GPUs to split the graph over (only for Graph Parallel training)
gp_gpus: null

####################################################

optim: 
  # S2EF 2M: 12, 30 | S2EF All: 1, 3
  max_epochs: 3 
  # learning rate will taper off to zero, so either
  # set many epochs, min_lr, or different scheduler
  scheduler: LambdaLR
  scheduler_params:
    lambda_type: cosine
    warmup_factor: 0.2
    warmup_epochs: 0.1
    lr_min_factor: 0.01
    # taper down lr as if we would train for that many epochs. Default -1
    lr_max_epochs: 30

val_max_iter: 1000
hide_eval_progressbar: False      
logger: wandb
wandb: True
wandb_run_name: null
wandb_group: null
slurm_job_id: null
model_name: ${model.name}
# wandb.watch gradients, activations, model parameters (bool)
watch_model: False

# variables we can access in our code
job_name: 'results'
# job_name: ${hydra:job.name}
config_name: ${hydra:job.config_name}
# Stores the command line arguments overrides
override_dirname: ${hydra:job.override_dirname}

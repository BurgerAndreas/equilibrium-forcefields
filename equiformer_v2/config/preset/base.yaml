# @package _global_
# ^^^ this @package directive solves any nesting problem (if this file is included in another folder)

model:
  name: equiformer_v2_oc20
  # FeedForwardNetwork, SO2EquivariantGraphAttention
  energy_head: FeedForwardNetwork

  # norm_type (str):Type of normalization layer (['layer_norm', 'layer_norm_sh', 'rms_norm_sh'])
  norm_type: 'layer_norm_sh' # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']
  normlayer_norm: norm # component, norm


  alpha_drop: 0.0 # [0.0, 0.1] Dropout rate for attention weights
  drop_path_rate: 0.0 # [0.0, 0.05, 0.1] Drop path rate
  # proj_drop (float): Dropout rate for outputs of attention and FFN in Transformer blocks
  proj_drop: 0.0
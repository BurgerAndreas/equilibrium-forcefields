# @package _global_
# this @package directive solves any nesting problem (if this file is included in another folder)

# call:
# scripts/deq_equiformer.py wandb=False ++preset=md17_aspirin_sel2.yaml

defaults:
  # if _self_ is the first entry, compositions will overwrite this config
  # if _self_ is the last entry, this config will overwrite compositions (default)
  # https://hydra.cc/docs/1.3/upgrades/1.0_to_1.1/default_composition_order/
  # - oc20
  # - ../../../equiformer/config/use/deq
  - _self_

override_test: 2

model_is_deq: True

model:
  name: deq_equiformer_v2_oc20
  num_layers: 1
  # weight, spectral, both, or none
  # https://github.com/locuslab/torchdeq/blob/4f6bd5fa66dd991cad74fcc847c88061764cf8db/torchdeq/norm/base_norm.py#L90C23-L90C32
  torchdeq_norm: 
    norm_type: weight_norm # None weight_norm, spectral_norm
    # dims (list or int, optional) – The dimensions along which to normalize.
    learn_scale: True # (bool, optional) – If true, learn a scale factor during training. Default True.
    target_norm: 1.0 # (float, optional) – The target norm value. Default 1.
    clip: False # (bool, optional) – If true, clip the scale factor. Default False.
    clip_value: 1.0 # (float, optional) – The value to clip the scale factor to. Default 1.

  # use_variational_alpha_drop: False
  # use_variational_path_drop: False

  # deq
  z0: "zero"
  # concat or add input injection to node_features (fixed-point estimate)
  # cat, add, lc (linear combination), cwlc (component wise lc = linear layer without bias)
  inp_inj: add
  inj_norm: prev # None=False, 'one', 'prev'
  
  path_norm: "none"
  irrep_norm: null

deq_kwargs_test:
  # on test set only when passed a fixed-point estimate
  fpreuse_f_max_iter: _default
  fpreuse_f_tol: _default
  # on test set
  f_max_iter: _default
  f_tol: _default
  f_stop_mode: _default

# if you want to specify a different solver for test set
# only works if evaluate is True
test_solver: null

# passed to torchdeq
deq_kwargs:
  # https://torchdeq.readthedocs.io/en/latest/torchdeq/core.html#torchdeq.core.get_deq
  core: sliced # sliced, indexing
  # 'broyden' anderson newton
  # (str, optional) – The forward solver function. Default 'fixed_point_iter'
  f_solver: 'anderson'
  # (str, optional) – The backward solver function. Default 'fixed_point_iter'.
  b_solver: 'anderson'
  # f_tol (float, optional) – The forward pass solver stopping criterion. Default 1e-3.
  f_tol: 1e-3
  # b_tol (float, optional) – The backward pass solver stopping criterion. Default 1e-6.
  b_tol: 1e-6
  # f_max_iter (int, optional) – Maximum number of iterations (NFE) for the forward solver. Default 40.
  f_max_iter: 40
  # b_max_iter (int, optional) – Maximum number of iterations (NFE) for the backward solver. Default 40
  b_max_iter: 40
  f_stop_mode: 'abs' # 'rel', 'abs'
  # ift (bool, optional) – If true, enable Implicit Differentiation. 
  # IFT=Implicit Function Theorem. Default False.
  ift: False
  hook_ift: False
  # grad (Union[int, list[int], tuple[int]], optional) 
  # Specifies the steps of PhantomGrad. It allows for using multiple values 
  # to represent different gradient steps in the sampled trajectory states. 
  # Default 1.
  grad: 1
  # Uniformly samples trajectory states from the solver. The backward passes of sampled states will be automactically tracked.
  n_states: 1 
  indexing: null

  force_train_mode: False
  
  # regularization
  # jac_reg: False
  # jac_loss_weight: 0.0
  # jac_loss_freq: 0.0
  # jac_incremental: 0.0
  # torchdeq.loss.jac_reg(f0, z0, vecs=1, create_graph=True)

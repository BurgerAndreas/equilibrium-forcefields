trainer: forces_v2

# ocp/data/s2ef/2M/train/data.0000.lmdb
# ocp/data/s2ef/all/val_id/data_log.0000.txt
dataset:
  # - src: datasets/oc20/s2ef/2M/train/
  # - src: ocp/data/s2ef/2M/train/
  - src: ocp/data/s2ef/200k/train/
    normalize_labels: True
    target_mean: -0.7554450631141663
    target_std: 2.887317180633545
    grad_target_mean: 0.0
    grad_target_std: 2.887317180633545
  # - src: datasets/oc20/s2ef/all/val_id/
  - src: ocp/data/s2ef/all/val_id/

# 8,634,242 parameters

logger: wandb
wandb: True

task:
  dataset: trajectory_lmdb_v2
  description: "Regressing to energies and forces for DFT trajectories from OCP"
  type: regression
  metric: force_mae
  labels:
    - potential energy
  grad_input: atomic forces
  train_on_free_atoms: True
  eval_on_free_atoms: True
  

hide_eval_progressbar: False


model:
  name: equiformer_v2_oc20
  
  use_pbc:                  True
  regress_forces:           True
  otf_graph:                True
  max_neighbors:            20
  max_radius:               12.0
  # Maximum atomic number
  max_num_elements:         90 

  # num_layers (int):             Number of layers in the GNN
  # sphere_channels (int):        Number of spherical channels (one set per resolution)
  # attn_hidden_channels (int): Number of hidden channels used during SO(2) graph attention
  # num_heads (int):            Number of attention heads
  # attn_alpha_head (int):      Number of channels for alpha vector in each attention head
  # attn_value_head (int):      Number of channels for value vector in each attention head
  # ffn_hidden_channels (int):  Number of hidden channels used during feedforward network
  # ln_type (str):            Type of normalization layer (['layer_norm', 'layer_norm_sh', 'rms_norm_sh'])
  num_layers:               4 # 8 12
  sphere_channels:          64 # 128
  attn_hidden_channels:     32              # [64, 96] This determines the hidden size of message passing. Do not necessarily use 96. 
  num_heads:                8 # 8
  attn_alpha_channels:      32  # 64            # Not used when `use_s2_act_attn` is True. 
  attn_value_channels:      8 # 16
  ffn_hidden_channels:      128 # 128
  ln_type:                'layer_norm_sh'    # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']

  # lmax_list (int):              List of maximum degree of the spherical harmonics (1 to 10)
  # mmax_list (int):              List of maximum order of the spherical harmonics (0 to lmax)
  # grid_resolution (int):        Resolution of SO3_Grid
  lmax_list:                [6]             # 4
  mmax_list:                [2]             
  grid_resolution:          14              # [18, 16, 14, None] For `None`, simply comment this line. 

  # num_sphere_samples (int):     Number of samples used to approximate the integration of the sphere in the output blocks
  num_sphere_samples:       128 # 128

  # edge_channels (int):                Number of channels for the edge invariant features
  edge_channels:              64 # 128
  # use_atom_edge_embedding (bool):     Whether to use atomic embedding along with relative distance for edge scalar features
  use_atom_edge_embedding:    True
  # share_atom_edge_embedding (bool):   Whether to share `atom_edge_embedding` across all blocks
  share_atom_edge_embedding:  False         # If `True`, `use_atom_edge_embedding` must be `True` and the atom edge embedding will be shared across all blocks. 
  # distance_function ("gaussian", "sigmoid", "linearsigmoid", "silu"):  Basis function used for distances
  distance_function:          'gaussian'
  num_distance_basis:         512           # not used
  # use_m_share_rad (bool):             Whether all m components within a type-L vector of one channel share radial function weights

  # attn_activation (str):      Type of activation function for SO(2) graph attention
  # use_s2_act_attn (bool):     Whether to use attention after S2 activation. Otherwise, use the same attention as Equiformer
  # use_attn_renorm (bool):     Whether to re-normalize attention weights
  # ffn_activation (str):       Type of activation function for feedforward network
  # use_gate_act (bool):        If `True`, use gate activation. Otherwise, use S2 activation
  # use_grid_mlp (bool):        If `True`, use projecting to grids and performing MLPs for FFNs.
  # use_sep_s2_act (bool):      If `True`, use separable S2 activation when `use_gate_act` is False.
  attn_activation:          'silu'
  use_s2_act_attn:          False       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention. 
  use_attn_renorm:          True        # Attention re-normalization. Used for ablation study.
  ffn_activation:           'silu'      # ['silu', 'swiglu']
  use_gate_act:             False       # [True, False] Switch between gate activation and S2 activation
  use_grid_mlp:             True        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.
  use_sep_s2_act:           True        # Separable S2 activation. Used for ablation study.

  # alpha_drop (float):         Dropout rate for attention weights
  alpha_drop:               0.1         # [0.0, 0.1]
  # path_drop (float):     Drop path rate
  path_drop:           0.05        # [0.0, 0.05, 0.1] 
  # proj_drop (float):          Dropout rate for outputs of attention and FFN in Transformer blocks
  proj_drop:                0.0

  # weight_init (str):          ['normal', 'uniform'] initialization of weights of linear layers except those in radial functions
  weight_init:              'uniform'    # ['uniform', 'normal']


optim:
  batch_size:                   1         # 4 6 8
  eval_batch_size:              2         # 6 # 12
  grad_accumulation_steps:      1         # gradient accumulation: effective batch size = `grad_accumulation_steps` * `batch_size` * (num of GPUs)
  load_balancing: atoms
  num_workers: 4 # 8
  lr_initial:                   0.0004    # [0.0002, 0.0004], eSCN uses 0.0008 for batch size 96
  
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: LambdaLR
  scheduler_params:
    lambda_type: cosine
    warmup_factor: 0.2
    warmup_epochs: 0.1
    lr_min_factor: 0.01         

  max_epochs: 30 # 3, 30
  force_coefficient: 100
  energy_coefficient: 2 # 4
  clip_grad_norm: 100
  ema_decay: 0.999
  loss_energy: mae
  loss_force: l2mae

  eval_every: 5000
output_dir: null
model_name: graph_attention_transformer_nonlinear_l2_md17
input_irreps: null
radius: 5.0
num_basis: 128
epochs: 1000
batch_size: 8
eval_batch_size: 24
model_ema: false
model_ema_decay: 0.9999
model_ema_force_cpu: false
drop_path: 0.0
opt: adamw
opt_eps: 1.0e-08
opt_betas: null
clip_grad: null
momentum: 0.9
weight_decay: 0.005
sched: cosine
lr: 0.0005
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
warmup_lr: 1.0e-06
min_lr: 1.0e-06
decay_epochs: 30
warmup_epochs: 10
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1
print_freq: 100
target: aspirin
data_path: datasets/md17
train_size: 950
val_size: 50
compute_stats: false
test_interval: 10
test_max_iter: 1000
energy_weight: 0.2
force_weight: 0.8
seed: 1
num_layers: 2
workers: 4
pin_mem: false
no_pin_mem: false
checkpoint_path: null
evaluate: false
meas_force: true
wandb: true
wandb_run_name: null
job_name: results
config_name: ${hydra:job.config_name}
override_dirname: ${hydra:job.override_dirname}

#!/bin/bash

# bash or sh
# Node resource configurations
# https://support.vectorinstitute.ai/Vaughan_slurm_changes
#SBATCH --job-name=equilibrium-forcefields
#SBATCH --mem=32G
#SBATCH --cpus-per-task=4
# a40, 48GB, gpu[001-015], gpu[027-056]
# rtx6000, 24GB
#SBATCH --partition=rtx6000
#SBATCH --gres=gpu:1
# --account=deadline --qos=deadline --qos=normal
#SBATCH --qos=deadline
#SBATCH --time=12:00:00

# Append is important because otherwise preemption resets the file
#SBATCH --open-mode=append

echo `date`: Job $SLURM_JOB_ID is allocated resource

# the recommendation is to keep erything that defines the workload itself in a separate script
# bash scripts/slurm_lora.sh
echo "Inside slurm_launcher.slrm ($0). received arguments: $@"

# export SCRIPTDIR=/h/burgeran/equilibrium-forcefields/equilibrium-forcefields/train
export SCRIPTDIR=/h/burgeran/equilibrium-forcefields/equiformer
if [[ $1 == *"test"* ]]; then
    echo "Found test in the filename. Changing the scriptdir to equilibrium-forcefields/tests"
    export SCRIPTDIR=/h/burgeran/equilibrium-forcefields/tests
elif [[ $1 == *"deq"* ]]; then
    echo "Found deq in the filename. Changing the scriptdir to equilibrium-forcefields/scripts"
    export SCRIPTDIR=/h/burgeran/equilibrium-forcefields/scripts
fi

# hand over all arguments to the script
echo "Submitting ${SCRIPTDIR}/$@"


# V1
# /h/burgeran/venv10/bin/python ${SCRIPTDIR}/"$@"

# V2 - works!
# source /h/burgeran/venv10/bin/activate
# /h/burgeran/venv10/bin/python ${SCRIPTDIR}/"$@"

# conda
# eval "$(mamba shell.bash hook)"
# source /h/burgeran/.bashrc
CONDA_BASE=$(conda info --base) # /fs01/home/burgeran/miniforge3
source $CONDA_BASE/etc/profile.d/conda.sh
mamba activate deq
/h/burgeran/miniforge3/envs/deq/bin/python ${SCRIPTDIR}/"$@"

# V3
# /h/burgeran/venv10/bin/accelerate-launch ${SCRIPTDIR}/"$@"

# V4
# source ~/venv10/bin/activate
# python ${SCRIPTDIR}/"$@"
# deactivate

echo `date`: "Job $SLURM_JOB_ID finished running, exit code: $?"

# date=$(date '+%Y-%m-%d')
# archive=$HOME/finished_jobs/$date/$SLURM_JOB_ID
# mkdir -p $archive

# echo archive $archive
# cp ./$SLURM_JOB_ID.out $archive/job.out
# cp ./$SLURM_JOB_ID.err $archive/job.err

# usage: 
# sbatch scripts/slurm_launcher.slrm deq_equiformer.py +machine=vector batch_size=16
